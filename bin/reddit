#!/usr/bin/env python

import re
from os import path
from subprocess import Popen
from urllib import urlopen
from zlib import adler32

from BeautifulSoup import BeautifulStoneSoup

fpath = path.expanduser("~/.redditurls")

DEFAULT_LIMIT = 25

def go(subreddit, limit, new):

    if not path.exists(fpath):
        with open(fpath, "w") as newf:
            newf.write('')

    cat = '' if not new else 'new/' # use hash table for others
    srch = re.compile('.+(https?://[a-zA-Z0-9./?=_-]+).+?\[link\]')
    soup = BeautifulStoneSoup(urlopen(
        'http://www.reddit.com/r/{}/{}.rss'.format(subreddit, cat)
        ))

    for i in soup.findAll('item', limit=limit):
        descr = i.description.text
        url = srch.search(descr).groups()[0]
        csum = adler32(url)

        f = open(fpath)
        if str(csum)+'\n' not in f:
            f.close()
            f = open(fpath, 'a')
            print >>f, csum
            cmd = 'open -a Safari {}'.format(url)
            p = Popen((cmd.split()))
            p.wait()
        f.close()


def usage():
    print """reddit [-n -c <count>] <SUBREDDIT> [SUBREDDIT...]"""


if __name__ == '__main__':

    import sys
    import getopt

    limit, new = DEFAULT_LIMIT, False

    try:
        opt, subreddits = getopt.getopt(sys.argv[1:], 'nc:')
        assert(subreddits)
        for o,a in opt:
            if o == '-c': limit = int(a)
            if o == '-n': new = True
    except:
        usage()
        raise SystemExit

    for subr in subreddits:
        go(subr, limit, new) # use kwargs

